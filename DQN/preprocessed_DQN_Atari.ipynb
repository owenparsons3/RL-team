{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/owenparsons3/RL-team/blob/main/preprocessed_DQN_Atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Overview\n",
        "\n",
        "This code is adapted from a YouTube Tutorial by brthor.\n",
        "(Source: https://www.youtube.com/watch?v=tsy1mgB7hB0&t=0s retrieved in April 2024.)\n",
        "\n",
        "Human level control through deep reinforcement learning (paper): DQN algorithm introduced by deepmind\n",
        "\n",
        "Replace the Q table in Q-learning with a deep neural network to estimate the Q-function. Q-function takes a state and returns the Q-value for each state action pair.\n",
        "\n",
        "For Atari games we have to do some preprocessing steps.\n",
        "\n",
        "we can also save after training and use that to render the agent later on (could save using msgpack or pickle)\n",
        "\n",
        "can plot pretty graphs using tensorboard"
      ],
      "metadata": {
        "id": "DK2UE8N_xdhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up"
      ],
      "metadata": {
        "id": "IVmQk9Jd2G2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"gym[atari, accept-rom-license]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mkmPfEzH-qh",
        "outputId": "5c2537a0-f273-4fbb-a266-21ccef9698a1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.66.2)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym.wrappers import FrameStack\n",
        "from gym.wrappers import GrayScaleObservation\n",
        "from gym.wrappers import ResizeObservation\n",
        "from collections import deque\n",
        "import itertools\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "# if GPU is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "cwyzu4vd2GX2"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ],
      "metadata": {
        "id": "_PPwAT4B9pwY"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameters\n",
        "GAMMA = TD target discount rate\n",
        "\n",
        "BATCH_SIZE = number of samples from replay buffer when computing gradients\n",
        "\n",
        "BUFFER = maximum number of transitions to store before updating old transitions\n",
        "\n",
        "REPLAY_MIN = minimum number of transition in the replay buffer before starting training\n",
        "\n",
        "EPS_START = start value for epsilon\n",
        "\n",
        "EPS_END = end value for epsilon EPS_START and EPS_END\n",
        "\n",
        "EPS_DECAY = number of steps between\n",
        "\n",
        "TARGET_UPDATE_FREQUENCY = frequency for updating the target parameters\n",
        "\n",
        "LR = learning rate"
      ],
      "metadata": {
        "id": "5jVn7_rIz9Bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "BUFFER = int(1e6)\n",
        "REPLAY_MIN = 50000\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.1\n",
        "EPS_DECAY = int(1e6)\n",
        "TARGET_UPDATE_FREQUENCY = 1000\n",
        "LR = 2.5e-4"
      ],
      "metadata": {
        "id": "UpNHZcFNxuDM"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Environment\n"
      ],
      "metadata": {
        "id": "ECQdzySh2pEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This class is required for PyTorch to be able to read the images from the game\n",
        "class TransposeImageObs(gym.ObservationWrapper):\n",
        "    def __init__(self, env, op):\n",
        "        super().__init__(env)\n",
        "        assert len(op) == 3, \"Op must have 3 dimensions\"\n",
        "\n",
        "        self.op = op\n",
        "\n",
        "        obs_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            self.observation_space.low[0, 0, 0],\n",
        "            self.observation_space.high[0, 0, 0],\n",
        "            [\n",
        "                obs_shape[self.op[0]],\n",
        "                obs_shape[self.op[1]],\n",
        "                obs_shape[self.op[2]]\n",
        "            ],\n",
        "            dtype=self.observation_space.dtype)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return obs.transpose(self.op[0], self.op[1], self.op[2])"
      ],
      "metadata": {
        "id": "fKIwc41wUFaK"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cnn architecture outline in the paper (which was published in the journal nature)\n",
        "def nature_cnn(observation_space, depths=(32, 64, 64), final_layer=512):\n",
        "  n_input_channels = observation_space.shape[0]\n",
        "\n",
        "  cnn = nn.Sequential(\n",
        "  nn.Conv2d(n_input_channels, depths[0], kernel_size=8, stride=4),\n",
        "  nn.ReLU(),\n",
        "  nn.Conv2d(depths[0], depths[1], kernel_size=4, stride=2),\n",
        "  nn.ReLU(),\n",
        "  nn.Conv2d(depths[1], depths[2], kernel_size=3, stride=1),\n",
        "  nn.ReLU(),\n",
        "  nn.Flatten()\n",
        "  )\n",
        "\n",
        "  #compute shape by doing one forward pass\n",
        "  with torch.no_grad():\n",
        "    n_flatten = cnn(torch.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
        "  out = nn.Sequential(cnn, nn.Linear(n_flatten, final_layer), nn.ReLU())\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "A1Zc4zF-L8Gv"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self, env, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_actions = env.action_space.n\n",
        "    self.device = device\n",
        "\n",
        "    conv_net = nature_cnn(env.observation_space)\n",
        "\n",
        "    self.net = nn.Sequential(\n",
        "        conv_net,\n",
        "        nn.Linear(512, self.num_actions)\n",
        "    )\n",
        "\n",
        "  #The forward function is required to run any PyTorch network\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "  def act(self, obs):\n",
        "    obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
        "    #obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device)\n",
        "    q_values = self(obs_t.unsqueeze(0)) #unsqueeze 0 to create a fake batch dimension because pytorch operations expect a batched dimension, we are not using a batched env\n",
        "    max_q_index = torch.argmax(q_values, dim=1)[0]\n",
        "    action = max_q_index.detach().item() #turn pytorch tensor into an integer using detach method\n",
        "\n",
        "    return action"
      ],
      "metadata": {
        "id": "RQle7_-6318-"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing"
      ],
      "metadata": {
        "id": "gIKIBCqedkXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a replay buffer that has a max length of BUFFER.\n",
        "\n",
        "Also create a reward buffer that stores the rewards for an episode to track training performance."
      ],
      "metadata": {
        "id": "z8mBfH8Q3zF-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4QU-p4FwvMa",
        "outputId": "19e62db2-2a9d-4f86-9f57-973a96885b14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(210, 160, 3)\n",
            "(84, 84, 3)\n",
            "(84, 84)\n",
            "(4, 84, 84)\n",
            "(4, 84, 84)\n",
            "(4, 84, 84)\n"
          ]
        }
      ],
      "source": [
        "#env = gym.make(\"CartPole-v1\")\n",
        "#may have to add preprocessing steps for this environment to work\n",
        "env = gym.make(\"ALE/Breakout-v5\")\n",
        "print(env.observation_space.shape)\n",
        "env = ResizeObservation(env, 84)\n",
        "print(env.observation_space.shape)\n",
        "env = GrayScaleObservation(env)\n",
        "print(env.observation_space.shape)\n",
        "env = FrameStack(env, 4)\n",
        "print(env.observation_space.shape)\n",
        "obs = env.reset()\n",
        "print(obs.shape)\n",
        "print(env.observation_space.shape)\n",
        "\n",
        "replay_buffer = deque(maxlen=BUFFER)\n",
        "reward_buffer = deque([0.0], maxlen=100)\n",
        "\n",
        "#there may be better ways to do this using gym and \"monitor\"\n",
        "episode_reward = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create neural networks"
      ],
      "metadata": {
        "id": "SeuXtj6o9Uc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "online_network = Network(env, device=device)\n",
        "target_network = Network(env, device=device)\n",
        "\n",
        "online_network =online_network.to(device)\n",
        "target_network = target_network.to(device)\n",
        "\n",
        "#optimise using the adam optimiser\n",
        "optimiser = torch.optim.Adam(online_network.parameters(), lr=LR)\n",
        "\n",
        "#set the target network parameters equal to the online network parameters (because they were initialised separately), this is also part of the algorithm in the paper\n",
        "target_network.load_state_dict(online_network.state_dict())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BuB3cM52nA2",
        "outputId": "13909fe3-178c-4d45-cafb-6f96dfbcd251"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialise replay buffer"
      ],
      "metadata": {
        "id": "e54fSs_J-ojz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initliase obs\n",
        "obs = env.reset()\n",
        "\n",
        "#get the first set of actions and observations before training using replay_min\n",
        "for _ in range(REPLAY_MIN):\n",
        "\n",
        "  #take an action\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "  #take a step in the environment based on the action and get teh new observations, reward and whether the episode is over\n",
        "  #store this information in the replay buffer\n",
        "  #set observation to the new observation\n",
        "  new_obs, reward, done, info = env.step(action)\n",
        "  transition = (obs, action, reward, done, new_obs)\n",
        "  replay_buffer.append(transition)\n",
        "  obs = new_obs\n",
        "\n",
        "  #reset the environment if the episode is over\n",
        "  if done:\n",
        "    obs = env.reset()"
      ],
      "metadata": {
        "id": "Fo-2WcMO-eqZ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "4vj0pDQwCPvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs = env.reset()\n",
        "\n",
        "for step in itertools.count():\n",
        "\n",
        "  #Epsilon greedy policy to facilitate exploration\n",
        "  epsilon = np.interp(step, [0, EPS_DECAY], [EPS_START, EPS_END])\n",
        "\n",
        "  sample = random.random()\n",
        "\n",
        "  if sample <= epsilon:\n",
        "    action = env.action_space.sample()\n",
        "  else:\n",
        "    action = online_network.act(obs)\n",
        "\n",
        "  # Interact with the environment every 4th frame\n",
        "  if step % 4 == 0:\n",
        "      # Take a step in the environment based on the action and get the new observations, reward, and whether the episode is over\n",
        "      # Store this information in the replay buffer\n",
        "      # Set observation to the new observation and add set reward to episode reward\n",
        "      new_obs, reward, done, info = env.step(action)\n",
        "      transition = (obs, action, reward, done, new_obs)\n",
        "      replay_buffer.append(transition)\n",
        "      obs = new_obs\n",
        "  else:\n",
        "      # Repeat the last action on the skipped frames\n",
        "      new_obs, reward, done, info = env.step(action)\n",
        "\n",
        "  episode_reward += reward\n",
        "\n",
        "  #reset the environment if the episode is over\n",
        "  if done:\n",
        "    obs = env.reset()\n",
        "\n",
        "    reward_buffer.append(episode_reward)\n",
        "    episode_reward = 0.0\n",
        "\n",
        "\n",
        "  #Start gradient steps\n",
        "  #####################\n",
        "  #sample BATCH_SIZE number of samples from the replay buffer\n",
        "  transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
        "\n",
        "  #separate transition tuple and use it to create lists for the batch and then convert to np (faster for converting to pytorch tensor)\n",
        "  obses = np.asarray([t[0] for t in transitions])\n",
        "  actions = np.asarray([t[1] for t in transitions])\n",
        "  rewards = np.asarray([t[2] for t in transitions])\n",
        "  rewards = np.clip(rewards, -1, 1) # preprocessing clipping rewards between -1 and 1 (While applying DQN to different environment settings,\n",
        "                                    # where reward points are not on the same scale, the training becomes inefficient.)\n",
        "  dones = np.asarray([t[3] for t in transitions])\n",
        "  new_obses = np.asarray([t[4] for t in transitions])\n",
        "\n",
        "  obses_t = torch.as_tensor(obses, dtype=torch.float32, device=device)\n",
        "  actions_t = torch.as_tensor(actions, dtype=torch.int64, device=device).unsqueeze(-1) #unsqeeze -1 adds dimension at the end\n",
        "  rewards_t = torch.as_tensor(rewards, dtype=torch.float32, device=device).unsqueeze(-1)\n",
        "  dones_t = torch.as_tensor(dones, dtype=torch.float32, device=device).unsqueeze(-1)\n",
        "  new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "  #Compute Targets\n",
        "  ################\n",
        "  target_q_values = target_network(new_obses_t)\n",
        "\n",
        "  #we want the highest q-value per observation\n",
        "  max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
        "\n",
        "  #if the episode is over (dones_t = 1) then we zero out the rest of the function only leaving the reward\n",
        "  targets = rewards_t + GAMMA * (1 - dones_t) * max_target_q_values\n",
        "\n",
        "  #Compute Loss\n",
        "  #############\n",
        "  q_values = online_network(obses_t)\n",
        "\n",
        "  #getting q_value for the action taken\n",
        "  action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
        "\n",
        "  #huber loss function (smooth_l1_loss in pytorch)\n",
        "  loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
        "\n",
        "  #Gradient Descent\n",
        "  ##################\n",
        "\n",
        "  optimiser.zero_grad()\n",
        "  loss.backward()\n",
        "  optimiser.step()\n",
        "\n",
        "  #update target network parameters\n",
        "  if step % TARGET_UPDATE_FREQUENCY == 0:\n",
        "    target_network.load_state_dict(online_network.state_dict())\n",
        "\n",
        "  #Logging\n",
        "  if step % 100 == 0:\n",
        "    reward_mean = np.mean(reward_buffer)\n",
        "    print()\n",
        "    print('Step', step)\n",
        "    print('Avg reward', reward_mean)\n",
        "\n",
        "    writer.add_scalar(\"Avg reward\", reward_mean, global_step = step)\n",
        "\n",
        "#Limits training\n",
        "  if step > 1500:\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "J1Hi2OERBR2Z",
        "outputId": "56df6590-6b68-4ba5-f4d0-1c91cce93d8d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 0\n",
            "Avg reward 0.0\n",
            "\n",
            "Step 100\n",
            "Avg reward 0.0\n",
            "\n",
            "Step 200\n",
            "Avg reward 0.5\n",
            "\n",
            "Step 300\n",
            "Avg reward 0.5\n",
            "\n",
            "Step 400\n",
            "Avg reward 0.6666666666666666\n",
            "\n",
            "Step 500\n",
            "Avg reward 0.6666666666666666\n",
            "\n",
            "Step 600\n",
            "Avg reward 0.6666666666666666\n",
            "\n",
            "Step 700\n",
            "Avg reward 1.25\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-f98f992389db>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mrewards_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0mdones_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mnew_obses_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_obses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "writer.flush()"
      ],
      "metadata": {
        "id": "WTRsEPjiZLGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer.close()"
      ],
      "metadata": {
        "id": "HKEraO5KEXuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "5N7prUhk-EWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "tAfwZeHf-MGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(online_network.state_dict(), \"trained_9000\")"
      ],
      "metadata": {
        "id": "2q7ti22euncG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Network(env, device)\n",
        "#net = net.to(device)\n",
        "\n",
        "net.load_state_dict(torch.load(\"trained_9000\"))"
      ],
      "metadata": {
        "id": "z3QnityWezue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()"
      ],
      "metadata": {
        "id": "_zwEvlmHfd6b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}