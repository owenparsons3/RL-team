{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/owenparsons3/RL-team/blob/main/preprocessed_DQN_Atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK2UE8N_xdhc"
      },
      "source": [
        "#Overview\n",
        "\n",
        "This code is adapted from a YouTube Tutorial by brthor.\n",
        "(Source: https://www.youtube.com/watch?v=tsy1mgB7hB0&t=0s retrieved in April 2024.)\n",
        "\n",
        "Human level control through deep reinforcement learning (paper): DQN algorithm introduced by deepmind\n",
        "\n",
        "Replace the Q table in Q-learning with a deep neural network to estimate the Q-function. Q-function takes a state and returns the Q-value for each state action pair.\n",
        "\n",
        "For Atari games we have to do some preprocessing steps.\n",
        "\n",
        "we can also save after training and use that to render the agent later on (could save using msgpack or pickle)\n",
        "\n",
        "can plot pretty graphs using tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVmQk9Jd2G2y"
      },
      "source": [
        "#Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cwyzu4vd2GX2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import FrameStack\n",
        "from gymnasium.wrappers import GrayScaleObservation\n",
        "from gymnasium.wrappers import ResizeObservation\n",
        "from collections import deque\n",
        "import itertools\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "# if GPU is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_PPwAT4B9pwY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.8/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  np.bool8: (False, True),\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:205: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  np.bool8: (False, True),\n",
            "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "/usr/local/lib/python3.8/dist-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
            "  'nearest': pil_image.NEAREST,\n",
            "/usr/local/lib/python3.8/dist-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
            "  'bilinear': pil_image.BILINEAR,\n",
            "/usr/local/lib/python3.8/dist-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
            "  'bicubic': pil_image.BICUBIC,\n",
            "/usr/local/lib/python3.8/dist-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
            "  'hamming': pil_image.HAMMING,\n",
            "/usr/local/lib/python3.8/dist-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
            "  'box': pil_image.BOX,\n",
            "/usr/local/lib/python3.8/dist-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
            "  'lanczos': pil_image.LANCZOS,\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jVn7_rIz9Bf"
      },
      "source": [
        "##Hyperparameters\n",
        "GAMMA = TD target discount rate\n",
        "\n",
        "BATCH_SIZE = number of samples from replay buffer when computing gradients\n",
        "\n",
        "BUFFER = maximum number of transitions to store before updating old transitions\n",
        "\n",
        "REPLAY_MIN = minimum number of transition in the replay buffer before starting training\n",
        "\n",
        "EPS_START = start value for epsilon\n",
        "\n",
        "EPS_END = end value for epsilon EPS_START and EPS_END\n",
        "\n",
        "EPS_DECAY = number of steps between\n",
        "\n",
        "TARGET_UPDATE_FREQUENCY = frequency for updating the target parameters\n",
        "\n",
        "LR = learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UpNHZcFNxuDM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "#Hyperparameters\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "BUFFER = int(1e6)\n",
        "REPLAY_MIN = 50000\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.1\n",
        "EPS_DECAY = int(1e6)\n",
        "TARGET_UPDATE_FREQUENCY = 1000\n",
        "LR = 2.5e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECQdzySh2pEo"
      },
      "source": [
        "#Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fKIwc41wUFaK"
      },
      "outputs": [],
      "source": [
        "#This class is required for PyTorch to be able to read the images from the game\n",
        "class TransposeImageObs(gym.ObservationWrapper):\n",
        "    def __init__(self, env, op):\n",
        "        super().__init__(env)\n",
        "        assert len(op) == 3, \"Op must have 3 dimensions\"\n",
        "\n",
        "        self.op = op\n",
        "\n",
        "        obs_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            self.observation_space.low[0, 0, 0],\n",
        "            self.observation_space.high[0, 0, 0],\n",
        "            [\n",
        "                obs_shape[self.op[0]],\n",
        "                obs_shape[self.op[1]],\n",
        "                obs_shape[self.op[2]]\n",
        "            ],\n",
        "            dtype=self.observation_space.dtype)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return obs.transpose(self.op[0], self.op[1], self.op[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A1Zc4zF-L8Gv"
      },
      "outputs": [],
      "source": [
        "#cnn architecture outline in the paper (which was published in the journal nature)\n",
        "def nature_cnn(observation_space, depths=(32, 64, 64), final_layer=512):\n",
        "  n_input_channels = observation_space.shape[0]\n",
        "\n",
        "  cnn = nn.Sequential(\n",
        "  nn.Conv2d(n_input_channels, depths[0], kernel_size=8, stride=4),\n",
        "  nn.ReLU(),\n",
        "  nn.Conv2d(depths[0], depths[1], kernel_size=4, stride=2),\n",
        "  nn.ReLU(),\n",
        "  nn.Conv2d(depths[1], depths[2], kernel_size=3, stride=1),\n",
        "  nn.ReLU(),\n",
        "  nn.Flatten()\n",
        "  )\n",
        "\n",
        "  #compute shape by doing one forward pass\n",
        "  with torch.no_grad():\n",
        "    n_flatten = cnn(torch.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
        "  out = nn.Sequential(cnn, nn.Linear(n_flatten, final_layer), nn.ReLU())\n",
        "\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RQle7_-6318-"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self, env, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_actions = env.action_space.n\n",
        "    self.device = device\n",
        "\n",
        "    conv_net = nature_cnn(env.observation_space)\n",
        "\n",
        "    self.net = nn.Sequential(\n",
        "        conv_net,\n",
        "        nn.Linear(512, self.num_actions)\n",
        "    )\n",
        "\n",
        "  #The forward function is required to run any PyTorch network\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "  def act(self, obs):\n",
        "    #obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
        "    obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device)\n",
        "    q_values = self(obs_t.unsqueeze(0)) #unsqueeze 0 to create a fake batch dimension because pytorch operations expect a batched dimension, we are not using a batched env\n",
        "    max_q_index = torch.argmax(q_values, dim=1)[0]\n",
        "    action = max_q_index.detach().item() #turn pytorch tensor into an integer using detach method\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIKIBCqedkXw"
      },
      "source": [
        "Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8mBfH8Q3zF-"
      },
      "source": [
        "Create a replay buffer that has a max length of BUFFER.\n",
        "\n",
        "Also create a reward buffer that stores the rewards for an episode to track training performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4QU-p4FwvMa",
        "outputId": "19e62db2-2a9d-4f86-9f57-973a96885b14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(210, 160, 3)\n",
            "(84, 84, 3)\n",
            "(84, 84)\n",
            "(4, 84, 84)\n"
          ]
        }
      ],
      "source": [
        "#env = gym.make(\"CartPole-v1\")\n",
        "#may have to add preprocessing steps for this environment to work\n",
        "env = gym.make(\"ALE/Breakout-v5\")\n",
        "print(env.observation_space.shape)\n",
        "env = ResizeObservation(env, 84)\n",
        "print(env.observation_space.shape)\n",
        "env = GrayScaleObservation(env)\n",
        "print(env.observation_space.shape)\n",
        "env = FrameStack(env, 4)\n",
        "print(env.observation_space.shape)\n",
        "#obs = env.reset()\n",
        "#print(obs.shape)\n",
        "#print(env.observation_space.shape)\n",
        "\n",
        "replay_buffer = deque(maxlen=BUFFER)\n",
        "reward_buffer = deque([0.0], maxlen=100)\n",
        "\n",
        "#there may be better ways to do this using gym and \"monitor\"\n",
        "episode_reward = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeuXtj6o9Uc_"
      },
      "source": [
        "#Create neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BuB3cM52nA2",
        "outputId": "13909fe3-178c-4d45-cafb-6f96dfbcd251"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "online_network = Network(env, device=device)\n",
        "target_network = Network(env, device=device)\n",
        "\n",
        "online_network =online_network.to(device)\n",
        "target_network = target_network.to(device)\n",
        "\n",
        "#optimise using the adam optimiser\n",
        "optimiser = torch.optim.Adam(online_network.parameters(), lr=LR)\n",
        "\n",
        "#set the target network parameters equal to the online network parameters (because they were initialised separately), this is also part of the algorithm in the paper\n",
        "target_network.load_state_dict(online_network.state_dict())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e54fSs_J-ojz"
      },
      "source": [
        "##Initialise replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Fo-2WcMO-eqZ"
      },
      "outputs": [],
      "source": [
        "# initliase obs\n",
        "obs = env.reset()\n",
        "\n",
        "#get the first set of actions and observations before training using replay_min\n",
        "for _ in range(REPLAY_MIN):\n",
        "\n",
        "  #take an action\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "  #take a step in the environment based on the action and get teh new observations, reward and whether the episode is over\n",
        "  #store this information in the replay buffer\n",
        "  #set observation to the new observation\n",
        "  new_obs, reward, done, _, info = env.step(action)\n",
        "  transition = (obs, action, reward, done, new_obs)\n",
        "  replay_buffer.append(transition)\n",
        "  obs = new_obs\n",
        "\n",
        "  #reset the environment if the episode is over\n",
        "  if done:\n",
        "    obs = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vj0pDQwCPvZ"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "J1Hi2OERBR2Z",
        "outputId": "56df6590-6b68-4ba5-f4d0-1c91cce93d8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0\n",
            "Avg reward 0.0\n",
            "\n",
            "Step 100\n",
            "Avg reward 0.0\n",
            "\n",
            "Step 200\n",
            "Avg reward 0.0\n",
            "\n",
            "Step 300\n",
            "Avg reward 0.0\n",
            "\n",
            "Step 400\n",
            "Avg reward 0.3333333333333333\n",
            "\n",
            "Step 500\n",
            "Avg reward 0.3333333333333333\n",
            "\n",
            "Step 600\n",
            "Avg reward 0.5\n",
            "\n",
            "Step 700\n",
            "Avg reward 0.6\n",
            "\n",
            "Step 800\n",
            "Avg reward 0.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-cb3215fe2bdc>:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
            "  obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 900\n",
            "Avg reward 0.6\n",
            "\n",
            "Step 1000\n",
            "Avg reward 1.0\n",
            "\n",
            "Step 1100\n",
            "Avg reward 1.0\n",
            "\n",
            "Step 1200\n",
            "Avg reward 1.0\n",
            "\n",
            "Step 1300\n",
            "Avg reward 1.0\n",
            "\n",
            "Step 1400\n",
            "Avg reward 1.125\n",
            "\n",
            "Step 1500\n",
            "Avg reward 1.125\n"
          ]
        }
      ],
      "source": [
        "obs = env.reset()\n",
        "\n",
        "for step in itertools.count():\n",
        "\n",
        "  #Epsilon greedy policy to facilitate exploration\n",
        "  epsilon = np.interp(step, [0, EPS_DECAY], [EPS_START, EPS_END])\n",
        "\n",
        "  sample = random.random()\n",
        "\n",
        "  if sample <= epsilon:\n",
        "    action = env.action_space.sample()\n",
        "  else:\n",
        "    action = online_network.act(obs)\n",
        "\n",
        "  # Interact with the environment every 4th frame\n",
        "  if step % 4 == 0:\n",
        "      # Take a step in the environment based on the action and get the new observations, reward, and whether the episode is over\n",
        "      # Store this information in the replay buffer\n",
        "      # Set observation to the new observation and add set reward to episode reward\n",
        "      new_obs, reward, done, _, info = env.step(action)\n",
        "      transition = (obs, action, reward, done, new_obs)\n",
        "      replay_buffer.append(transition)\n",
        "      obs = new_obs\n",
        "  else:\n",
        "      # Repeat the last action on the skipped frames\n",
        "      new_obs, reward, done, _, info = env.step(action)\n",
        "\n",
        "  episode_reward += reward\n",
        "\n",
        "  #reset the environment if the episode is over\n",
        "  if done:\n",
        "    obs = env.reset()\n",
        "\n",
        "    reward_buffer.append(episode_reward)\n",
        "    episode_reward = 0.0\n",
        "\n",
        "\n",
        "  #Start gradient steps\n",
        "  #####################\n",
        "  #sample BATCH_SIZE number of samples from the replay buffer\n",
        "  transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
        "\n",
        "  #separate transition tuple and use it to create lists for the batch and then convert to np (faster for converting to pytorch tensor)\n",
        "  #obses = np.asarray([np.array(t[0]) for t in transitions]) #this line breaks when converting to array for some reason\n",
        "  obslist = []\n",
        "# Print shapes of individual frames\n",
        "  for t in transitions:\n",
        "      if np.array(t[0], dtype = object).shape != (4, 84, 84):\n",
        "          obslist.append(np.array(t[0][0]))\n",
        "      else:\n",
        "          obslist.append(np.array(t[0]))\n",
        "  obses = np.array(obslist)\n",
        "  \n",
        "  actions = np.asarray([t[1] for t in transitions])\n",
        "  rewards = np.asarray([t[2] for t in transitions])\n",
        "  rewards = np.clip(rewards, -1, 1) # preprocessing clipping rewards between -1 and 1 (While applying DQN to different environment settings,\n",
        "                                    # where reward points are not on the same scale, the training becomes inefficient.)\n",
        "  dones = np.asarray([t[3] for t in transitions])\n",
        "  new_obses = np.asarray([t[4] for t in transitions])\n",
        "\n",
        "  obses_t = torch.as_tensor(obses, dtype=torch.float32, device=device)\n",
        "  actions_t = torch.as_tensor(actions, dtype=torch.int64, device=device).unsqueeze(-1) #unsqeeze -1 adds dimension at the end\n",
        "  rewards_t = torch.as_tensor(rewards, dtype=torch.float32, device=device).unsqueeze(-1)\n",
        "  dones_t = torch.as_tensor(dones, dtype=torch.float32, device=device).unsqueeze(-1)\n",
        "  new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "  #Compute Targets\n",
        "  ################\n",
        "  target_q_values = target_network(new_obses_t)\n",
        "\n",
        "  #we want the highest q-value per observation\n",
        "  max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
        "\n",
        "  #if the episode is over (dones_t = 1) then we zero out the rest of the function only leaving the reward\n",
        "  targets = rewards_t + GAMMA * (1 - dones_t) * max_target_q_values\n",
        "\n",
        "  #Compute Loss\n",
        "  #############\n",
        "  q_values = online_network(obses_t)\n",
        "\n",
        "  #getting q_value for the action taken\n",
        "  action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
        "\n",
        "  #huber loss function (smooth_l1_loss in pytorch)\n",
        "  loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
        "\n",
        "  #Gradient Descent\n",
        "  ##################\n",
        "\n",
        "  optimiser.zero_grad()\n",
        "  loss.backward()\n",
        "  optimiser.step()\n",
        "\n",
        "  #update target network parameters\n",
        "  if step % TARGET_UPDATE_FREQUENCY == 0:\n",
        "    target_network.load_state_dict(online_network.state_dict())\n",
        "\n",
        "  #Logging\n",
        "  if step % 100 == 0:\n",
        "    reward_mean = np.mean(reward_buffer)\n",
        "    print()\n",
        "    print('Step', step)\n",
        "    print('Avg reward', reward_mean)\n",
        "\n",
        "    writer.add_scalar(\"Avg reward\", reward_mean, global_step = step)\n",
        "\n",
        "#Limits training\n",
        "  if reward_mean > 8:\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WTRsEPjiZLGw"
      },
      "outputs": [],
      "source": [
        "writer.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HKEraO5KEXuw"
      },
      "outputs": [],
      "source": [
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2q7ti22euncG"
      },
      "outputs": [],
      "source": [
        "torch.save(online_network.state_dict(), \"trained_reward_8\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
